"""
Manejador de LLM simplificado para producciÃ³n - Solo OpenRouter
"""

import os
import logging
from typing import Optional, List, Dict, Any
import aiohttp
import asyncio

from llm_config_production import llm_config

logger = logging.getLogger(__name__)

class ProductionLLMHandler:
    """Maneja las llamadas a OpenRouter de forma optimizada para producciÃ³n"""
    
    def __init__(self):
        self.config = llm_config.get_config()
        
    async def get_response(self, message: str, context: Optional[List[Dict]] = None) -> str:
        """
        Obtiene una respuesta de OpenRouter
        
        Args:
            message: El mensaje del usuario
            context: Historial de conversaciÃ³n opcional
            
        Returns:
            La respuesta del LLM
        """
        
        if not self.config.get("api_key"):
            return "âŒ Bot no configurado correctamente. Contacta al administrador."
        
        logger.info(f"ğŸ¤– Procesando mensaje: {message[:100]}...")
        
        try:
            return await self._call_openrouter(message, context)
                
        except Exception as e:
            logger.error(f"âŒ Error llamando a OpenRouter: {e}")
            return "ğŸ˜… Disculpa, tuve un problema tÃ©cnico. Â¿PodrÃ­as repetir tu pregunta?"
    
    async def _call_openrouter(self, message: str, context: Optional[List[Dict]] = None) -> str:
        """Llama a la API de OpenRouter con manejo robusto de errores"""
        
        headers = {
            "Authorization": f"Bearer {self.config['api_key']}",
            "HTTP-Referer": self.config.get("site_url", "https://slack.com"),
            "X-Title": self.config.get("app_name", "Dona Bot"),
            "Content-Type": "application/json"
        }
        
        # Construir mensajes
        messages = [{"role": "system", "content": llm_config.system_prompt}]
        if context:
            messages.extend(context)
        messages.append({"role": "user", "content": message})
        
        data = {
            "model": self.config["model"],
            "messages": messages,
            "max_tokens": self.config["max_tokens"],
            "temperature": self.config["temperature"]
        }
        
        # Timeout mÃ¡s largo para requests en producciÃ³n
        timeout = aiohttp.ClientTimeout(total=30)
        
        try:
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers=headers,
                    json=data
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        response_text = result["choices"][0]["message"]["content"]
                        logger.info("âœ… Respuesta recibida de OpenRouter")
                        return response_text
                    else:
                        error_text = await response.text()
                        logger.error(f"âŒ Error HTTP {response.status}: {error_text}")
                        
                        # Respuestas especÃ­ficas segÃºn el error
                        if response.status == 429:
                            return "â³ El servicio estÃ¡ muy ocupado. Intenta de nuevo en unos segundos."
                        elif response.status == 401:
                            return "ğŸ” Error de autenticaciÃ³n. Contacta al administrador."
                        else:
                            return "ğŸ”§ Error del servicio. Intenta de nuevo mÃ¡s tarde."
                            
        except asyncio.TimeoutError:
            logger.error("â° Timeout en request a OpenRouter")
            return "â° La respuesta estÃ¡ tardando mucho. Intenta con una pregunta mÃ¡s simple."
        except Exception as e:
            logger.error(f"ğŸ’¥ Error inesperado: {e}")
            return "ğŸ’¥ Error inesperado. Intenta de nuevo."

# Para uso sÃ­ncrono en el bot
def get_llm_response_sync(message: str, context: Optional[List[Dict]] = None) -> str:
    """VersiÃ³n sÃ­ncrona optimizada para producciÃ³n"""
    handler = ProductionLLMHandler()
    
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(handler.get_response(message, context))
        finally:
            loop.close()
    except Exception as e:
        logger.error(f"ğŸ’¥ Error en wrapper sÃ­ncrono: {e}")
        return "ğŸ˜… Error interno. Por favor intenta de nuevo."